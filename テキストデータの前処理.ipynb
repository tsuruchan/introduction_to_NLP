{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 自然言語処理 入門\n",
    "# テキストデータの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 自然言語処理とは\n",
    "人間が日常的に使っている「自然言語」をコンピュータに処理させる一連の技術のことです。\n",
    "\n",
    "__自然言語処理における機械学習のフロー__\n",
    "![](https://qiita-image-store.s3.amazonaws.com/0/77079/824d44eb-6499-add8-73ed-2aa923d02bc8.png)\n",
    "\n",
    "\n",
    "自然言語処理をやろうと思うと、__前処理（Pre-processing）__が今まで以上に大切になります。\n",
    "\n",
    "テキストの状態のデータは文字の羅列であり構造化されていません。それを今までのように数値データのベクトルに変換しなければなりません。\n",
    "さらに、ノイズがあったり文字の長さが均一では無いので、上手に前処理をしないと精度が出ないことが多いです。\n",
    "\n",
    "ですので今回は、__テキストデータの前処理__を主に勉強していきます。\n",
    "\n",
    "## 前処理の主な流れ\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 単語の分割\n",
    "自然言語処理では、__文章（テキスト）を単語レベルに区切って扱う__ことがほとんどです。ベクトル化する為には、テキストを分割するという前準備が必要になります。\n",
    "<br>\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-2.png)\n",
    "\n",
    "ここで、日本語特有の問題が発生します。英語は、単語の区切りが空白で示されていますが、__日本語には区切りが無い__ということです。\n",
    "\n",
    "日本語のように単語の区切りが明らかでない言語に対して行われるのが、単語の分割をする処理です。今回は__「形態素解析」__と__「N-gram」__という代表的な2つの手法を解説していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 形態素解析\n",
    "#### 「形態素」とは？\n",
    "文章を、それ以上分解したら意味をなさなくなる要素にまで分割したものです。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-3.png)\n",
    "\n",
    "このように、形態素で分割したものを__わかち書き__と言います。\n",
    "\n",
    "pythonには、形態素解析をしてくれる__MeCab__というパッケージがあるので、早速インストールしてみましょう。\n",
    "\n",
    "インストールは以下のコマンド一つで完了します。（macの場合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mecab-python3 in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install mecab-python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "では早速、MeCabを使ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "東京\t名詞,固有名詞,地域,一般,*,*,東京,トウキョウ,トーキョー\n",
      "都\t名詞,接尾,地域,*,*,*,都,ト,ト\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "住ん\t動詞,自立,*,*,五段・マ行,連用タ接続,住む,スン,スン\n",
      "で\t助詞,接続助詞,*,*,*,*,で,デ,デ\n",
      "ます\t助動詞,*,*,*,特殊・マス,基本形,ます,マス,マス\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "text = \"私は東京都に住んでます\"\n",
    "\n",
    "# parse()の引数に文字列を入力すると解析結果が返ってきます。\n",
    "print(mecab.parse(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私 は 東京 都 に 住ん で ます \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Taggerの引数を\"-Owakati\"にすると、わかち書きされたテキストが返されます\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "text = \"私は東京都に住んでます\"\n",
    "\n",
    "print(mecab.parse(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### N-gram\n",
    "N文字単位で文字列を分解する処理を行います。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# 文字列を受け取り、指定されたN-gramのリストを返す関数\n",
    "\n",
    "def ngram(words,N):\n",
    "    ngram = []\n",
    "\n",
    "    for i in range(len(words)):\n",
    "        cw = \"\"\n",
    "        if i >= N-1:\n",
    "            for j in reversed(range(N)):\n",
    "                cw += words[i-j]\n",
    "        else:\n",
    "            continue\n",
    "        ngram.append(cw)\n",
    "\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私', 'は', '東', '京', '都', 'に', '住', 'ん', 'で', 'い', 'ま', 'す']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(\"私は東京都に住んでいます\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['私は', 'は東', '東京', '京都', '都に', 'に住', '住ん', 'んで', 'でい', 'いま', 'ます']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram(\"私は東京都に住んでいます\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "以上で、最初のステップである「単語の分割」が終わりました。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-1.png)\n",
    "\n",
    "次からは、「単語の選択」を見ていきたいと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 単語の選択\n",
    "\n",
    "### ステミング\n",
    "主に英語などの欧米系の言語では、意味的には同じ単語が活用形などの影響で違う文字列になっていることがあります。\n",
    "さらに、複数形や単数形といったような違いもしばしばあります。\n",
    "\n",
    "ステミングは、単語からすべての接辞を取り除く処理を行います。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-5.png)\n",
    "\n",
    "ステミングをするには、NLTKパッケージをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages\n",
      "Requirement already up-to-date: six in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages (from nltk)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "program\n",
      "initi\n",
      "initi\n",
      "initi\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# programへステミング\n",
    "print(stemmer.stem('programming'))\n",
    "print(stemmer.stem('programs'))\n",
    "\n",
    "# initiへステミング\n",
    "print(stemmer.stem('initial'))\n",
    "print(stemmer.stem('initialize'))\n",
    "print(stemmer.stem('initialization'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### ストップワードの除去\n",
    "トップワードというのは自然言語処理する際に一般的で役に立たない等の理由で処理対象外とする単語のことです。\n",
    "\n",
    "たとえば、助詞や助動詞(「は」「の」「です」「ます」など)が挙げられます。これらの単語は出現頻度が高い割に有効な情報をもたらしません。さらに、計算量や性能に悪影響を及ぼすこともあるので除去するのが一般的です。\n",
    "\n",
    "今回は以下の方式で、ストップワードの除去をしていきます。\n",
    "- 辞書による方式\n",
    "- 出現頻度による方式\n",
    "\n",
    "#### 辞書によるストップワードの除去\n",
    "あらかじめストップワードの辞書（リスト）を作成しておき、辞書内に含まれる単語をテキストから除去します。\n",
    "\n",
    "その際に、既に公開されている日本語のストップワード辞書のひとつである、[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を使うのがいいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "slothlib_path = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
    "slothlib_file = urllib.request.urlopen(slothlib_path)\n",
    "slothlib_stopwords = [line.decode(\"utf-8\").strip() for line in slothlib_file]\n",
    "slothlib_stopwords = [ss for ss in slothlib_stopwords if not ss==u'']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あそこ',\n",
       " 'あたり',\n",
       " 'あちら',\n",
       " 'あっち',\n",
       " 'あと',\n",
       " 'あな',\n",
       " 'あなた',\n",
       " 'あれ',\n",
       " 'いくつ',\n",
       " 'いつ',\n",
       " 'いま',\n",
       " 'いや',\n",
       " 'いろいろ',\n",
       " 'うち',\n",
       " 'おおまか',\n",
       " 'おまえ',\n",
       " 'おれ',\n",
       " 'がい',\n",
       " 'かく',\n",
       " 'かたち',\n",
       " 'かやの',\n",
       " 'から',\n",
       " 'がら',\n",
       " 'きた',\n",
       " 'くせ',\n",
       " 'ここ',\n",
       " 'こっち',\n",
       " 'こと',\n",
       " 'ごと',\n",
       " 'こちら',\n",
       " 'ごっちゃ',\n",
       " 'これ',\n",
       " 'これら',\n",
       " 'ごろ',\n",
       " 'さまざま',\n",
       " 'さらい',\n",
       " 'さん',\n",
       " 'しかた',\n",
       " 'しよう',\n",
       " 'すか',\n",
       " 'ずつ',\n",
       " 'すね',\n",
       " 'すべて',\n",
       " 'ぜんぶ',\n",
       " 'そう',\n",
       " 'そこ',\n",
       " 'そちら',\n",
       " 'そっち',\n",
       " 'そで',\n",
       " 'それ',\n",
       " 'それぞれ',\n",
       " 'それなり',\n",
       " 'たくさん',\n",
       " 'たち',\n",
       " 'たび',\n",
       " 'ため',\n",
       " 'だめ',\n",
       " 'ちゃ',\n",
       " 'ちゃん',\n",
       " 'てん',\n",
       " 'とおり',\n",
       " 'とき',\n",
       " 'どこ',\n",
       " 'どこか',\n",
       " 'ところ',\n",
       " 'どちら',\n",
       " 'どっか',\n",
       " 'どっち',\n",
       " 'どれ',\n",
       " 'なか',\n",
       " 'なかば',\n",
       " 'なに',\n",
       " 'など',\n",
       " 'なん',\n",
       " 'はじめ',\n",
       " 'はず',\n",
       " 'はるか',\n",
       " 'ひと',\n",
       " 'ひとつ',\n",
       " 'ふく',\n",
       " 'ぶり',\n",
       " 'べつ',\n",
       " 'へん',\n",
       " 'ぺん',\n",
       " 'ほう',\n",
       " 'ほか',\n",
       " 'まさ',\n",
       " 'まし',\n",
       " 'まとも',\n",
       " 'まま',\n",
       " 'みたい',\n",
       " 'みつ',\n",
       " 'みなさん',\n",
       " 'みんな',\n",
       " 'もと',\n",
       " 'もの',\n",
       " 'もん',\n",
       " 'やつ',\n",
       " 'よう',\n",
       " 'よそ',\n",
       " 'わけ',\n",
       " 'わたし',\n",
       " 'ハイ',\n",
       " '上',\n",
       " '中',\n",
       " '下',\n",
       " '字',\n",
       " '年',\n",
       " '月',\n",
       " '日',\n",
       " '時',\n",
       " '分',\n",
       " '秒',\n",
       " '週',\n",
       " '火',\n",
       " '水',\n",
       " '木',\n",
       " '金',\n",
       " '土',\n",
       " '国',\n",
       " '都',\n",
       " '道',\n",
       " '府',\n",
       " '県',\n",
       " '市',\n",
       " '区',\n",
       " '町',\n",
       " '村',\n",
       " '各',\n",
       " '第',\n",
       " '方',\n",
       " '何',\n",
       " '的',\n",
       " '度',\n",
       " '文',\n",
       " '者',\n",
       " '性',\n",
       " '体',\n",
       " '人',\n",
       " '他',\n",
       " '今',\n",
       " '部',\n",
       " '課',\n",
       " '係',\n",
       " '外',\n",
       " '類',\n",
       " '達',\n",
       " '気',\n",
       " '室',\n",
       " '口',\n",
       " '誰',\n",
       " '用',\n",
       " '界',\n",
       " '会',\n",
       " '首',\n",
       " '男',\n",
       " '女',\n",
       " '別',\n",
       " '話',\n",
       " '私',\n",
       " '屋',\n",
       " '店',\n",
       " '家',\n",
       " '場',\n",
       " '等',\n",
       " '見',\n",
       " '際',\n",
       " '観',\n",
       " '段',\n",
       " '略',\n",
       " '例',\n",
       " '系',\n",
       " '論',\n",
       " '形',\n",
       " '間',\n",
       " '地',\n",
       " '員',\n",
       " '線',\n",
       " '点',\n",
       " '書',\n",
       " '品',\n",
       " '力',\n",
       " '法',\n",
       " '感',\n",
       " '作',\n",
       " '元',\n",
       " '手',\n",
       " '数',\n",
       " '彼',\n",
       " '彼女',\n",
       " '子',\n",
       " '内',\n",
       " '楽',\n",
       " '喜',\n",
       " '怒',\n",
       " '哀',\n",
       " '輪',\n",
       " '頃',\n",
       " '化',\n",
       " '境',\n",
       " '俺',\n",
       " '奴',\n",
       " '高',\n",
       " '校',\n",
       " '婦',\n",
       " '伸',\n",
       " '紀',\n",
       " '誌',\n",
       " 'レ',\n",
       " '行',\n",
       " '列',\n",
       " '事',\n",
       " '士',\n",
       " '台',\n",
       " '集',\n",
       " '様',\n",
       " '所',\n",
       " '歴',\n",
       " '器',\n",
       " '名',\n",
       " '情',\n",
       " '連',\n",
       " '毎',\n",
       " '式',\n",
       " '簿',\n",
       " '回',\n",
       " '匹',\n",
       " '個',\n",
       " '席',\n",
       " '束',\n",
       " '歳',\n",
       " '目',\n",
       " '通',\n",
       " '面',\n",
       " '円',\n",
       " '玉',\n",
       " '枚',\n",
       " '前',\n",
       " '後',\n",
       " '左',\n",
       " '右',\n",
       " '次',\n",
       " '先',\n",
       " '春',\n",
       " '夏',\n",
       " '秋',\n",
       " '冬',\n",
       " '一',\n",
       " '二',\n",
       " '三',\n",
       " '四',\n",
       " '五',\n",
       " '六',\n",
       " '七',\n",
       " '八',\n",
       " '九',\n",
       " '十',\n",
       " '百',\n",
       " '千',\n",
       " '万',\n",
       " '億',\n",
       " '兆',\n",
       " '下記',\n",
       " '上記',\n",
       " '時間',\n",
       " '今回',\n",
       " '前回',\n",
       " '場合',\n",
       " '一つ',\n",
       " '年生',\n",
       " '自分',\n",
       " 'ヶ所',\n",
       " 'ヵ所',\n",
       " 'カ所',\n",
       " '箇所',\n",
       " 'ヶ月',\n",
       " 'ヵ月',\n",
       " 'カ月',\n",
       " '箇月',\n",
       " '名前',\n",
       " '本当',\n",
       " '確か',\n",
       " '時点',\n",
       " '全部',\n",
       " '関係',\n",
       " '近く',\n",
       " '方法',\n",
       " '我々',\n",
       " '違い',\n",
       " '多く',\n",
       " '扱い',\n",
       " '新た',\n",
       " 'その後',\n",
       " '半ば',\n",
       " '結局',\n",
       " '様々',\n",
       " '以前',\n",
       " '以後',\n",
       " '以降',\n",
       " '未満',\n",
       " '以上',\n",
       " '以下',\n",
       " '幾つ',\n",
       " '毎日',\n",
       " '自体',\n",
       " '向こう',\n",
       " '何人',\n",
       " '手段',\n",
       " '同じ',\n",
       " '感じ']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ストップワードリスト\n",
    "\n",
    "slothlib_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 出現頻度による除去\n",
    "テキスト内の全ての単語出現頻度をカウントし、高頻度の単語をテキストから除去する方法です。以下の図は、ある英語の本の最も頻出する50単語の累計頻度をプロットしたものです。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-6.png)\n",
    "\n",
    "50単語を見てみると、theやof、カンマのような文書分類等に役に立たなそうな単語がテキストの50%近くを占めていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "以上で、「単語の選択」が終わりました。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-1.png)\n",
    "\n",
    "最後に、「ベクトル化」について勉強していきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## ベクトル化\n",
    "Bag-of-Words とは、文章に単語が含まれているかどうかのみを考え、単語の並び方などは考慮しないモデルのことです。一番シンプルなモデルは単語があれば 1、なければ 0 となります。\n",
    "\n",
    "まずは、テキストをわかち書きして、出現した単語をリスト化します。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-7.png)\n",
    "\n",
    "次に、各文章に対して、リストの単語があれば１、なければ０を入れます。この形式をBag of Wordsと呼びます。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-1-8.png)\n",
    "\n",
    "以上で、テキストがベクトル形式になったので、これを特徴量として機械学習をしていくことが出来るようになりました。\n",
    "\n",
    "次回は、もう一つのベクトル化手法であるword2vecというものについて勉強していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
