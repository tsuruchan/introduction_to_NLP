{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 自然言語処理 入門\n",
    "# word2vecによる自然言語処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Bag of Wordsの問題点としてあげられるのは、__文の構造は全て無視して、どの単語が含まれているかだけを見ること__です。さらに、ベクトルの次元数は単語数と同じになるため、テキストによっては数百万次元などになってしまう可能性もあります。\n",
    "\n",
    "例えば、以下のような単語があったとすると、Bag of Wordsではこのような表現の仕方になります。\n",
    "\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-2.png)\n",
    "（ちなみに、このようなベクトルのことをone-hotベクトルと呼びます。）\n",
    "これだと、各ベクトル間の距離は一定になります。（距離＝$\\sqrt 2$）\n",
    "\n",
    "そこで先ほど挙げた問題点を解決するには、ベクトルの次元数を固定して、かつ__似ている単語は距離が近くなるようなベクトル表現__を実現するための手法が必要になります。それが今回扱う__word2vec__というものです。\n",
    "\n",
    "__似ている単語は距離が近くなるようなベクトル表現__というもののイメージを持つために、図で解説します。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-1.png)\n",
    "青文字で表されている物は、果物なので意味的には近いので、ベクトル空間上でも近い距離にいます。しかし、青文字で表されている物とオレンジ色で表されている物の間には意味的な近さは無いので、ベクトル空間上でも離れた位置にあります。\n",
    "\n",
    "\n",
    "さらに、距離を近くすると同時に、単語の抽象度を上げていきます。そうすることによって、表現力豊かな分散表現を得ることができます。その結果、単語の加減算ができるようになります。\n",
    "\n",
    "例えば、「King - Man + Woman」つまり「大様から男を引いて女を足した物は何か？」という計算が出来るようになります。結果は、queenです。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-3.png)\n",
    "\n",
    "word2vecの目的は、以上のような__表現力の高い、単語の分散表現をつくること__です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## word2vecの手法\n",
    "今回は、word2vecで分散表現を求める、代表的な手法を２つ紹介します。\n",
    "\n",
    "### Continuous Bag of Words (CBOW)\n",
    "Continuous Bag of Words (CBOW)は、単語$w_t$の前後の単語（$w_{t-n}, ..., w_{t+n}$）を入力として、単語$w_t$を出力とするようなニューラルネットワークのことです。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-4.png)\n",
    "\n",
    "単語の個数nはオプションで指定できます。デフォルトではn=5になっていることが多いです。\n",
    "\n",
    "つまりCBOWの処理は、__文脈中の単語から対象単語が現れる条件付き確率を最大化すること__です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Skip-gram\n",
    "Skip-gramは、CBOWと対照的に、単語$w_t$が与えられて、文脈中の1単語$w_{t+k}$を推定するアルゴリズムです。\n",
    "\n",
    "学習過程では、入力層に単語$w_t$を入れ、正解データとして単語$w_{t+k}$を入れることを繰り返していきます。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-5.png)\n",
    "\n",
    "つまり、Skip-gramは__出力層における周辺単語予測のエラー率の合計を最小化すること__です。\n",
    "\n",
    "\n",
    "### CBOWとSkip-gram、どちらを使えばいいのか？\n",
    "Skip-gramは学習データが少なくてもある程度の精度がでるとされています。 そして、CBoWよりもSkip-gramの方が実験では意味的な精度が高くなり、構文の精度ではあまり大差はないようです。\n",
    "\n",
    "以上より、総合的に__Skip-gram__の方を使うのがよいでしょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Gensimのインストール\n",
    "word2vecをpythonで扱える__gensim__というライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages/gensim-2.0.0-py3.5-macosx-10.7-x86_64.egg\n",
      "Requirement already up-to-date: numpy>=1.3 in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Collecting scipy>=0.7.0 (from gensim)\n",
      "  Downloading scipy-0.19.0-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (16.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 16.1MB 41kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.5.0 in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages (from gensim)\n",
      "Requirement already up-to-date: smart_open>=1.2.1 in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages/smart_open-1.5.2-py3.5.egg (from gensim)\n",
      "Requirement already up-to-date: boto>=2.32 in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages/boto-2.46.1-py3.5.egg (from smart_open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: bz2file in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages/bz2file-0.98-py3.5.egg (from smart_open>=1.2.1->gensim)\n",
      "Requirement already up-to-date: requests in /Users/tsuruoka/.pyenv/versions/anaconda3-2.4.0/envs/jupyter/lib/python3.5/site-packages/requests-2.13.0-py3.5.egg (from smart_open>=1.2.1->gensim)\n",
      "Installing collected packages: scipy\n",
      "  Found existing installation: scipy 0.18.1\n",
      "    Uninstalling scipy-0.18.1:\n",
      "      Successfully uninstalled scipy-0.18.1\n",
      "Successfully installed scipy-0.19.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### word2vecの学習済みモデルのダウンロード\n",
    "今回は、googleが公開している学習済みモデルをつかって、word2vecを使っていきます。\n",
    "\n",
    "[DLリンク](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n",
    "\n",
    "「GoogleNews-vectors-negative300.bin.gz」というファイルをダウンロードして、解凍します。その後、そのファイルをこのnotebookと同じディレクトリに配置します。\n",
    "\n",
    "※ このファイルは解凍前で1.65GB、解凍後は3.64GBになります。PCの残り容量にご注意ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# googleのword2vecモデルを読み込む\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "__most_similarメソッド__を使うと、意味が近い単語を表示してくれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lives', 0.6027060747146606),\n",
       " ('chiseled_burnished_refined', 0.5310098528862),\n",
       " ('Interaction_enriches_your', 0.4961995482444763),\n",
       " ('Shanda_Interaction_enriches', 0.49457842111587524),\n",
       " ('Life', 0.4814741611480713),\n",
       " ('Fayaz_Wani_reports', 0.4636700749397278),\n",
       " ('lifestyle', 0.44911327958106995),\n",
       " ('joys_sorrows', 0.44626227021217346),\n",
       " ('humdrum_existence', 0.44414764642715454),\n",
       " ('earthly_existence', 0.44378799200057983)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['life'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "さらに、positive引数に単語をリストで入れると、単語どうしの足し算をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meats', 0.7095532417297363),\n",
       " ('chicken', 0.6442906260490417),\n",
       " ('cheese', 0.64256751537323),\n",
       " ('stewing_steak', 0.6253078579902649),\n",
       " ('potatoes', 0.6224780678749084),\n",
       " ('sausages', 0.6213147044181824),\n",
       " ('presliced', 0.6200767755508423),\n",
       " ('breads', 0.6197592616081238),\n",
       " ('pasta', 0.6131477355957031),\n",
       " ('pork_sausages', 0.6109585762023926)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['bread', 'meat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "先ほどの、「King - Man + Woman」つまり「王様から男を引いて女を足した物は何か？」という計算が出来るようになったかどうか、試してみましょう。\n",
    "![](https://s3.amazonaws.com/ai-standard/nlp-2-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.4827325940132141),\n",
       " ('queens', 0.4667814075946808),\n",
       " ('kumaris', 0.4653734564781189),\n",
       " ('kings', 0.45586392283439636),\n",
       " ('womens', 0.422832190990448),\n",
       " ('princes', 0.4176960587501526),\n",
       " ('Al_Anqari', 0.41725507378578186),\n",
       " ('concubines', 0.4011078178882599),\n",
       " ('monarch', 0.39624831080436707),\n",
       " ('monarchy', 0.39430150389671326)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['king', 'women'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "> ('queen', 0.4827325940132141)\n",
    "\n",
    "見ての通り、意味を捉えた分散表現が学習できていることがわかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 単語の分散表現を取得\n",
    "word2vecにより、学習した単語の分散表現を取得できるようになりました。表示の仕方は、pyhtonのディクショナリ形式です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.25976562e-01,   2.97851562e-02,   8.60595703e-03,\n",
       "         1.39648438e-01,  -2.56347656e-02,  -3.61328125e-02,\n",
       "         1.11816406e-01,  -1.98242188e-01,   5.12695312e-02,\n",
       "         3.63281250e-01,  -2.42187500e-01,  -3.02734375e-01,\n",
       "        -1.77734375e-01,  -2.49023438e-02,  -1.67968750e-01,\n",
       "        -1.69921875e-01,   3.46679688e-02,   5.21850586e-03,\n",
       "         4.63867188e-02,   1.28906250e-01,   1.36718750e-01,\n",
       "         1.12792969e-01,   5.95703125e-02,   1.36718750e-01,\n",
       "         1.01074219e-01,  -1.76757812e-01,  -2.51953125e-01,\n",
       "         5.98144531e-02,   3.41796875e-01,  -3.11279297e-02,\n",
       "         1.04492188e-01,   6.17675781e-02,   1.24511719e-01,\n",
       "         4.00390625e-01,  -3.22265625e-01,   8.39843750e-02,\n",
       "         3.90625000e-02,   5.85937500e-03,   7.03125000e-02,\n",
       "         1.72851562e-01,   1.38671875e-01,  -2.31445312e-01,\n",
       "         2.83203125e-01,   1.42578125e-01,   3.41796875e-01,\n",
       "        -2.39257812e-02,  -1.09863281e-01,   3.32031250e-02,\n",
       "        -5.46875000e-02,   1.53198242e-02,  -1.62109375e-01,\n",
       "         1.58203125e-01,  -2.59765625e-01,   2.01416016e-02,\n",
       "        -1.63085938e-01,   1.35803223e-03,  -1.44531250e-01,\n",
       "        -5.68847656e-02,   4.29687500e-02,  -2.46582031e-02,\n",
       "         1.85546875e-01,   4.47265625e-01,   9.58251953e-03,\n",
       "         1.31835938e-01,   9.86328125e-02,  -1.85546875e-01,\n",
       "        -1.00097656e-01,  -1.33789062e-01,  -1.25000000e-01,\n",
       "         2.83203125e-01,   1.23046875e-01,   5.32226562e-02,\n",
       "        -1.77734375e-01,   8.59375000e-02,  -2.18505859e-02,\n",
       "         2.05078125e-02,  -1.39648438e-01,   2.51464844e-02,\n",
       "         1.38671875e-01,  -1.05468750e-01,   1.38671875e-01,\n",
       "         8.88671875e-02,  -7.51953125e-02,  -2.13623047e-02,\n",
       "         1.72851562e-01,   4.63867188e-02,  -2.65625000e-01,\n",
       "         8.91113281e-03,   1.49414062e-01,   3.78417969e-02,\n",
       "         2.38281250e-01,  -1.24511719e-01,  -2.17773438e-01,\n",
       "        -1.81640625e-01,   2.97851562e-02,   5.71289062e-02,\n",
       "        -2.89306641e-02,   1.24511719e-02,   9.66796875e-02,\n",
       "        -2.31445312e-01,   5.81054688e-02,   6.68945312e-02,\n",
       "         7.08007812e-02,  -3.08593750e-01,  -2.14843750e-01,\n",
       "         1.45507812e-01,  -4.27734375e-01,  -9.39941406e-03,\n",
       "         1.54296875e-01,  -7.66601562e-02,   2.89062500e-01,\n",
       "         2.77343750e-01,  -4.86373901e-04,  -1.36718750e-01,\n",
       "         3.24218750e-01,  -2.46093750e-01,  -3.03649902e-03,\n",
       "        -2.11914062e-01,   1.25000000e-01,   2.69531250e-01,\n",
       "         2.04101562e-01,   8.25195312e-02,  -2.01171875e-01,\n",
       "        -1.60156250e-01,  -3.78417969e-02,  -1.20117188e-01,\n",
       "         1.15234375e-01,  -4.10156250e-02,  -3.95507812e-02,\n",
       "        -8.98437500e-02,   6.34765625e-03,   2.03125000e-01,\n",
       "         1.86523438e-01,   2.73437500e-01,   6.29882812e-02,\n",
       "         1.41601562e-01,  -9.81445312e-02,   1.38671875e-01,\n",
       "         1.82617188e-01,   1.73828125e-01,   1.73828125e-01,\n",
       "        -2.37304688e-01,   1.78710938e-01,   6.34765625e-02,\n",
       "         2.36328125e-01,  -2.08984375e-01,   8.74023438e-02,\n",
       "        -1.66015625e-01,  -7.91015625e-02,   2.43164062e-01,\n",
       "        -8.88671875e-02,   1.26953125e-01,  -2.16796875e-01,\n",
       "        -1.73828125e-01,  -3.59375000e-01,  -8.25195312e-02,\n",
       "        -6.49414062e-02,   5.07812500e-02,   1.35742188e-01,\n",
       "        -7.47070312e-02,  -1.64062500e-01,   1.15356445e-02,\n",
       "         4.45312500e-01,  -2.15820312e-01,  -1.11328125e-01,\n",
       "        -1.92382812e-01,   1.70898438e-01,  -1.25000000e-01,\n",
       "         2.65502930e-03,   1.92382812e-01,  -1.74804688e-01,\n",
       "         1.39648438e-01,   2.92968750e-01,   1.13281250e-01,\n",
       "         5.95703125e-02,  -6.39648438e-02,   9.96093750e-02,\n",
       "        -2.72216797e-02,   1.96533203e-02,   4.27246094e-02,\n",
       "        -2.46093750e-01,   6.39648438e-02,  -2.25585938e-01,\n",
       "        -1.68945312e-01,   2.89916992e-03,   8.20312500e-02,\n",
       "         3.41796875e-01,   4.32128906e-02,   1.32812500e-01,\n",
       "         1.42578125e-01,   7.61718750e-02,   5.98144531e-02,\n",
       "        -1.19140625e-01,   2.74658203e-03,  -6.29882812e-02,\n",
       "        -2.72216797e-02,  -4.82177734e-03,  -8.20312500e-02,\n",
       "        -2.49023438e-02,  -4.00390625e-01,  -1.06933594e-01,\n",
       "         4.24804688e-02,   7.76367188e-02,  -1.16699219e-01,\n",
       "         7.37304688e-02,  -9.22851562e-02,   1.07910156e-01,\n",
       "         1.58203125e-01,   4.24804688e-02,   1.26953125e-01,\n",
       "         3.61328125e-02,   2.67578125e-01,  -1.01074219e-01,\n",
       "        -3.02734375e-01,  -5.76171875e-02,   5.05371094e-02,\n",
       "         5.26428223e-04,  -2.07031250e-01,  -1.38671875e-01,\n",
       "        -8.97216797e-03,  -2.78320312e-02,  -1.41601562e-01,\n",
       "         2.07031250e-01,  -1.58203125e-01,   1.27929688e-01,\n",
       "         1.49414062e-01,  -2.24609375e-02,  -8.44726562e-02,\n",
       "         1.22558594e-01,   2.15820312e-01,  -2.13867188e-01,\n",
       "        -3.12500000e-01,  -3.73046875e-01,   4.08935547e-03,\n",
       "         1.07421875e-01,   1.06933594e-01,   7.32421875e-02,\n",
       "         8.97216797e-03,  -3.88183594e-02,  -1.29882812e-01,\n",
       "         1.49414062e-01,  -2.14843750e-01,  -1.83868408e-03,\n",
       "         9.91210938e-02,   1.57226562e-01,  -1.14257812e-01,\n",
       "        -2.05078125e-01,   9.91210938e-02,   3.69140625e-01,\n",
       "        -1.97265625e-01,   3.54003906e-02,   1.09375000e-01,\n",
       "         1.31835938e-01,   1.66992188e-01,   2.35351562e-01,\n",
       "         1.04980469e-01,  -4.96093750e-01,  -1.64062500e-01,\n",
       "        -1.56250000e-01,  -5.22460938e-02,   1.03027344e-01,\n",
       "         2.43164062e-01,  -1.88476562e-01,   5.07812500e-02,\n",
       "        -9.37500000e-02,  -6.68945312e-02,   2.27050781e-02,\n",
       "         7.61718750e-02,   2.89062500e-01,   3.10546875e-01,\n",
       "        -5.37109375e-02,   2.28515625e-01,   2.51464844e-02,\n",
       "         6.78710938e-02,  -1.21093750e-01,  -2.15820312e-01,\n",
       "        -2.73437500e-01,  -3.07617188e-02,  -3.37890625e-01,\n",
       "         1.53320312e-01,   2.33398438e-01,  -2.08007812e-01,\n",
       "         3.73046875e-01,   8.20312500e-02,   2.51953125e-01,\n",
       "        -7.61718750e-02,  -4.66308594e-02,  -2.23388672e-02,\n",
       "         2.99072266e-02,  -5.93261719e-02,  -4.66918945e-03,\n",
       "        -2.44140625e-01,  -2.09960938e-01,  -2.87109375e-01,\n",
       "        -4.54101562e-02,  -1.77734375e-01,  -2.79296875e-01,\n",
       "        -8.59375000e-02,   9.13085938e-02,   2.51953125e-01], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['king']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "単語の分散表現を得られることによって、これを特徴量としたデータセットをつくることも可能です。\n",
    "\n",
    "<br>\n",
    "\n",
    "### word2vecを使用した特徴量について\n",
    "実際にテキストを特徴量にしようとしたときに躓く問題があります。それは、特徴量にしたい物は単語ではなく、一文の文章であったり、長い記事のテキストを特徴量に変換したい場面が多いでしょう。\n",
    "\n",
    "word2vecは、単語をベクトルにすることは出来ますが、文章をベクトルにすることはできません。\n",
    "\n",
    "仮に、文章をわかちがきして、その単語をベクトル化したものを連結させても、テキストの長さや単語数がバラバラなので、特徴量が一定の形にならないでしょう。\n",
    "\n",
    "\n",
    "では、どうすればいいか。\n",
    "\n",
    "まず一つ目には、特徴量の形を、単語数が最大のテキストに合わせ、それに満たない場合の値はすべて０で埋めるという方式です。CNNの時に勉強した、0-paddingのようなイメージです。\n",
    "\n",
    "参考になる記事を載せておきます。\n",
    "\n",
    "[畳み込みニューラルネットワークによる文書分類](http://qiita.com/ichiroex/items/f225f6d8eceb6796cc7e)\n",
    "\n",
    "<br>\n",
    "\n",
    "２つ目は、Doc2vecと呼ばれるアルゴリズムを使用する方法です。\n",
    "Doc2vecはword2vecの派生形で、Doc＝文章をベクトル化してくれるアルゴリズムです。\n",
    "\n",
    "こちらも、参考になる記事を載せておきます。\n",
    "\n",
    "[Doc2Vecの仕組みとgensimを使った文書類似度算出チュートリアル](https://deepage.net/machine_learning/2017/01/08/doc2vec.html)\n",
    "\n",
    "\n",
    "その他にも、次回やるRNNの特徴量として単語の分散表現を使う方法などがあるでしょう。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
